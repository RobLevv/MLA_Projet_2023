{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook MLA Project - Fader Networks\n",
    "\n",
    "**Authors:** Adrien PETARD, Robin LEVEQUE, Th√©o MAGOUDI, Eliot CHRISTON\n",
    "\n",
    "**Group:** 11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Imports](#2-imports)\n",
    "3. [Annotations](#3-annotations)\n",
    "    - [Identity](#identity)\n",
    "    - [Attributes](#attributes)\n",
    "    - [Bounding Boxes](#bounding-boxes)\n",
    "    - [Landmarks](#landmarks)\n",
    "4. [Evaluation](#4-evaluation)\n",
    "5. [Images](#5-images)\n",
    "6. [Annexes](#6-annexes)\n",
    "    - [6.1 How to connect the jupyter server and Vscode, and how to use/understand it:](#61-how-to-connect-the-jupyter-server-and-vscode-and-how-to-useunderstand-it)\n",
    "    - [6.2 Our point of comparison is the Fader Networks paper (https://arxiv.org/pdf/1706.00409.pdf)](#62-our-point-of-comparison-is-the-fader-networks-paper-httpsarxivorgpdf170600409pdf)\n",
    "    - [6.3 Train your own models](#63-train-your-own-models)\n",
    "        - [6.3.1 Train a classifier](#631-train-a-classifier)\n",
    "        - [6.3.2 Train a fader Network](#632-train-a-fader-network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The goal of this notebook is to explore the CelebA dataset and to understand how it is structured.\n",
    "\n",
    "Here is the link to the dataset: [https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\n",
    "\n",
    "The data is divided into 3 folders:\n",
    "- <u>**Anno** (`annotation`):</u> contains the annotations of the dataset\n",
    "- <u>**Eval** (`evaluation`):</u> contains the evaluation files of the dataset\n",
    "- <u>**Img** (`images`):</u> contains the images of the dataset, here we chose to use the aligned and cropped images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "## 3. Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Identity\n",
    "\n",
    "The identity is a number representing the person in the image. Each person has a unique identity number but can appear in multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Anno/identity_CelebA.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m identity \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/Anno/identity_CelebA.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m identity\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m identity\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Anno/identity_CelebA.txt'"
     ]
    }
   ],
   "source": [
    "%cd ../..\n",
    "identity = pd.read_csv('data/Anno/identity_CelebA.txt', sep=\" \", header=None, index_col=0)\n",
    "identity.columns = [\"identity_id\"]\n",
    "identity.index.name = \"image_id\"\n",
    "\n",
    "print(\"The shape of the identity dataframe is: \", identity.shape)\n",
    "display(identity.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will the identities according to their frequency\n",
    "identity_counts = identity.groupby(\"identity_id\").size().reset_index(name=\"count\") # group by identity_id and count the number of occurences\n",
    "identity_counts = identity_counts.sort_values(by=\"count\", ascending=False) # sort by count in descending order\n",
    "identity_counts = identity_counts.reset_index(drop=True) # reset the index\n",
    "\n",
    "print(\"Number of identities: {}\".format(len(identity_counts)), '\\n')\n",
    "\n",
    "print(\"Most frequent identities: \")\n",
    "display(identity_counts.head()) # first 5 (default) most frequent identities\n",
    "print(\"Least frequent identities: \")\n",
    "display(identity_counts.tail()) # last 5 (default) least frequent identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of appearances of identities\n",
    "\n",
    "identity_counts[\"count\"].plot.hist(\n",
    "    bins=100, \n",
    "    figsize=(10, 5), \n",
    "    title=\"Distribution of appearances of identities\",\n",
    "    xlabel=\"Number of appearances\",\n",
    "    ylabel=\"Number of identities\",\n",
    "    grid=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identities are mostly represented 30 times in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv('data/Anno/list_attr_celeba.txt', sep=\" \", header=1, index_col=0)\n",
    "attributes.index.name = \"image_id\"\n",
    "print(\"The shape of the attributes dataframe is: \", attributes.shape)\n",
    "print(\"Number of attributes: {}\".format(len(attributes.columns)))\n",
    "display(attributes.head()) # first 5 (default) attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_attributes = attributes.replace(to_replace=-1, value=0) # replace -1 with 0\n",
    "attribute_counts = binary_attributes.sum(axis=0).sort_values(ascending=False) # sum each column and sort\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(attribute_counts)), attribute_counts, color=\"orange\", alpha=0.5)\n",
    "plt.xlabel(\"Attribute\")\n",
    "plt.ylabel(\"Count\")\n",
    "# on the x axis, we need to label the bars with the attribute names\n",
    "plt.xticks(range(len(attribute_counts)), attribute_counts.index, rotation=90)\n",
    "plt.grid(True, axis=\"y\", alpha=0.5)\n",
    "plt.title(\"Attribute Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Bounding Boxes\n",
    "\n",
    "The bounding boxes are the coordinates of the face in the image. They are represented by 4 numbers: x, y, width and height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = pd.read_csv('data/Anno/list_bbox_celeba.txt', sep=\" \", header=1, index_col=0)\n",
    "bounding_boxes.index.name = \"image_id\"\n",
    "print(\"The shape of the bounding boxes dataframe is: \", bounding_boxes.shape)\n",
    "display(bounding_boxes.head()) # first 5 (default) bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Landmarks\n",
    "\n",
    "The landmarks are the coordinates of 5 points on the face: left eye, right eye, nose, left mouth and right mouth.\n",
    "\n",
    "There are 2 data_files for the landmarks: `list_landmarks_align_celeba.txt` and `list_landmarks_celeba.txt`. The first one contains the landmarks for the aligned images and the second one for the non-aligned images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_aligned = pd.read_csv('data/Anno/list_landmarks_align_celeba.txt', sep=\" \", header=1, index_col=0)\n",
    "landmarks_aligned.index.name = \"image_id\"\n",
    "print(\"The shape of the landmarks_aligned dataframe is: \", landmarks_aligned.shape)\n",
    "display(landmarks_aligned.head()) # first 5 (default) landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_wild = pd.read_csv('data/Anno/list_landmarks_celeba.txt', sep=\" \", header=1, index_col=0)\n",
    "landmarks_wild.index.name = \"image_id\"\n",
    "print(\"The shape of the landmarks_wild dataframe is: \", landmarks_wild.shape)\n",
    "display(landmarks_wild.head()) # first 5 (default) landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 4. Evaluation\n",
    "\n",
    "The dataset is divided into 3 parts: train, validation and test. The train and validation sets are used to train the model and the test set is used to evaluate the model.\n",
    "\n",
    "The split is defined in the `list_eval_partition.txt` file. It contains the identity number and the split number (0 for train, 1 for validation and 2 for test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_eval_partition = pd.read_csv('data/Eval/list_eval_partition.txt', sep=\" \", index_col=0)\n",
    "list_eval_partition.index.name = \"image_id\"\n",
    "list_eval_partition.columns = [\"partition\"]\n",
    "print(\"The shape of the list_eval_partition dataframe is: \", list_eval_partition.shape)\n",
    "display(list_eval_partition.head()) # first 5 (default) partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(list_eval_partition[list_eval_partition[\"partition\"] == 0])\n",
    "n_val = len(list_eval_partition[list_eval_partition[\"partition\"] == 1])\n",
    "n_test = len(list_eval_partition[list_eval_partition[\"partition\"] == 2])\n",
    "\n",
    "print(\"Number of training images: {}\".format(n_train))\n",
    "print(\"Number of validation images: {}\".format(n_val))\n",
    "print(\"Number of test images: {}\".format(n_test))\n",
    "\n",
    "# plot camembert\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(\n",
    "    [n_train, n_val, n_test], \n",
    "    labels=[\"Train\", \"Validation\", \"Test\"], \n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    textprops={'fontsize': 14}\n",
    ")\n",
    "plt.title(\"Partition Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 5. Images\n",
    "\n",
    "The Img folder contains all the images of the dataset. let's take a look at some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_images = 10\n",
    "fig, axes = plt.subplots(3, nb_images, figsize=(20, 8))\n",
    "list_random_images = list_eval_partition.sample(nb_images).index\n",
    "for i, image_id in enumerate(list_random_images):\n",
    "    img = plt.imread(\"data/Img/\" + image_id)\n",
    "    axes[0][i].imshow(img)\n",
    "    axes[0][i].axis(\"off\")\n",
    "    axes[0][i].set_title(image_id)\n",
    "    \n",
    "    # get the attributes of the image and display them\n",
    "    attributes = binary_attributes.loc[image_id]\n",
    "    list_attributes = attributes[attributes == 1].index\n",
    "    \n",
    "    axes[1][i].axis(\"off\")\n",
    "    axes[1][i].text(0, 0.5, \"\\n\".join(list_attributes), fontsize=8, ha=\"left\", va=\"center\")\n",
    "    \n",
    "    # adding identity\n",
    "    identity_id = identity.loc[image_id].values[0]\n",
    "    axes[2][i].imshow(img)\n",
    "    axes[2][i].axis(\"off\")\n",
    "    axes[2][i].set_title(\"identity_id: {}\".format(identity_id))\n",
    "    \n",
    "    # adding landmarks\n",
    "    landmarks = landmarks_aligned.loc[image_id]\n",
    "    axes[2][i].scatter(landmarks[0::2], landmarks[1::2], marker=\"+\", c=\"green\")\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 6. Annexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 How to connect the jupyter server and Vscode, and how to use/understand it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this link you will find all the instructions to complete the quest of connecting your notebook to the university's server: https://code.visualstudio.com/docs/datascience/jupyter-notebooks\n",
    "\n",
    "But to help you I will resume the steps you will go through:\n",
    "- Click on the kernel you're actually using on the top right corner of the vscode window\n",
    "- Click with the left click of your mouse adnd then choose \"Select another kernel\"\n",
    "And then enter all the informations needed:\n",
    "\n",
    "URL:    https://sdi.ppi.ingenierie.upmc.fr/gpu11\n",
    "\n",
    "ID:     notregroupe\n",
    "\n",
    "PWD:    notregroupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When you are on Vscode and you create a notebook, you have the option to choose a server to which you will connect. Here, the advantage is to connect to the university server to have access to the images without downloading them, and that on our favorite environment. We can then create our notebooks or even Python files that will be executed by the notebook which is relatively educational and allows improving the communication of information and progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the % or ! in front of a command that we would usually use in the terminal, we can execute it directly in the notebook:\n",
    "%cd\n",
    "!ls\n",
    "%ls\n",
    "\n",
    "# Now we are going to get the files we are interested in:\n",
    "%cd tests/MLA_Projet_2023/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Our point of comparison is the Fader Networks paper (https://arxiv.org/pdf/1706.00409.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to preprocess the images, that is to say:\n",
    "\n",
    "%cd\n",
    "%ll\n",
    "print(\"\\nstart point:\")\n",
    "%cd tests/MLA_Projet_2023/FaderNetworks_NIPS2017/\n",
    "# !pip install -r requirements.txt  #Uncomment this line if you don't have the required packages installed\n",
    "\n",
    "print(\"\\nthe datas we are going to use for this part:\")\n",
    "%cd data\n",
    "%ls\n",
    "\n",
    "# DECOMMENT THIS PART IF YOU NEED TO DOWNLOAD PREPROCESSED DATA\n",
    "# !chmod +x preprocess.py # We need to make the file executable and give it the right to be executed\n",
    "# !./preprocess.py\n",
    "\n",
    "# We have in VScode the file preprocess.py, we will use it to preprocess the images:\n",
    "\n",
    "# It will resize images, and create 2 files: images_256_256.pth and attributes.pth.\n",
    "# The first one contains a tensor of size (202599, 3, 256, 256) containing the concatenation of all resized images. Note that you can update the image size in preprocess.py to work with different resolutions.\n",
    "# The second file is a pre-processed version of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/notregroupe\n",
      "/home/notregroupe/tests/MLA_Projet_2023/FaderNetworks_NIPS2017/models\n",
      "autoencoder_12_16_01-09-43.pt  discriminator_12_16_01-09-43.pt  narrow_eyes.pth\n",
      "classifier128.pth              download.sh                      pointy_nose.pth\n",
      "classifier256.pth              eyeglasses.pth                   young.pth\n",
      "\u001b[0m\u001b[01;34mdefault\u001b[0m/                       male.pth\n"
     ]
    }
   ],
   "source": [
    "%cd\n",
    "%cd tests/MLA_Projet_2023/FaderNetworks_NIPS2017/models\n",
    "%ls\n",
    "\n",
    "# DECOMMENT THIS PART IF YOU WANT TO DOWNLOAD THE PRETRAINED MODELS\n",
    "# !chmod +x download.sh\n",
    "# !./download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a trained model, you can use it to swap attributes of images in the dataset. Below are examples using the pretrained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/notregroupe\n",
      "/home/notregroupe/tests/MLA_Projet_2023/FaderNetworks_NIPS2017\n"
     ]
    }
   ],
   "source": [
    "%cd\n",
    "%cd tests/MLA_Projet_2023/FaderNetworks_NIPS2017/\n",
    "if input(\"Do you want to download the pretrained models? (yes/no)\") == \"yes\":\n",
    "    !chmod +x download.sh\n",
    "    !./download.sh\n",
    "\n",
    "if input(\"Do you want to preprocess the images? (yes/no)\") == \"yes\":\n",
    "    # Narrow Eyes\n",
    "    !python interpolate.py --model_path models/narrow_eyes.pth --n_images 10 --n_interpolations 10 --alpha_min 10.0 --alpha_max 10.0 --output_path narrow_eyes.png\n",
    "\n",
    "    # Eyeglasses\n",
    "    !python interpolate.py --model_path models/eyeglasses.pth --n_images 10 --n_interpolations 10 --alpha_min 2.0 --alpha_max 2.0 --output_path eyeglasses.png\n",
    "\n",
    "    # Age\n",
    "    !python interpolate.py --model_path models/young.pth --n_images 10 --n_interpolations 10 --alpha_min 10.0 --alpha_max 10.0 --output_path young.png\n",
    "\n",
    "    # Gender\n",
    "    !python interpolate.py --model_path models/male.pth --n_images 10 --n_interpolations 10 --alpha_min 2.0 --alpha_max 2.0 --output_path male.png\n",
    "\n",
    "    # Pointy nose\n",
    "    !python interpolate.py --model_path models/pointy_nose.pth --n_images 10 --n_interpolations 10 --alpha_min 10.0 --alpha_max 10.0 --output_path pointy_nose.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd\n",
    "%cd tests/MLA_Projet_2023/FaderNetworks_NIPS2017/\n",
    "# Narrow Eyes\n",
    "from IPython.display import Image\n",
    "Image(filename='narrow_eyes.png')\n",
    "\n",
    "# Eyeglasses\n",
    "Image(filename='eyeglasses.png')\n",
    "\n",
    "# Age\n",
    "Image(filename='young.png')\n",
    "\n",
    "# Male\n",
    "Image(filename='male.png')\n",
    "\n",
    "# Pointy nose\n",
    "Image(filename='pointy_nose.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands will generate images with 10 rows of 12 columns with the interpolated images. The first column corresponds to the original image, the second is the reconstructed image (without alteration of the attribute), and the remaining ones correspond to the interpolated images. alpha_min and alpha_max represent the range of the interpolation. Values superior to 1 represent generations over the True / False range of the boolean attribute in the model. Note that the variations of some attributes may only be noticeable for high values of alphas. For instance, for the \"eyeglasses\" or \"gender\" attributes, alpha_max=2 is usually enough, while for the \"age\" or \"narrow eyes\" attributes, it is better to go up to alpha_max=10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Train your own models\n",
    "\n",
    "### 6.3.1 Train a classifier\n",
    "To train your own model you first need to train a classifier to let the model evaluate the swap quality during the training. Training a good classifier is relatively simple for most attributes, and a good model can be trained in a few minutes. We provide a trained classifier for all attributes in models/classifier256.pth. Note that the classifier does not need to be state-of-the-art, it is not used during the training process, but is just here to monitor the swap quality. If you want to train your own classifier, you can run classifier.py, using the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECOMMENT THIS PART TO USE THE CLASSIFIER\n",
    "if input(\"Do you want to use the classifier? (yes/no)\") == \"yes\":\n",
    "    %cd\n",
    "    %cd tests/MLA_Projet_2023/FaderNetworks_NIPS2017/\n",
    "    !chmod +x classifier.py\n",
    "    !python classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Main parameters\n",
    "--img_sz 256                  # image size\n",
    "--img_fm 3                    # number of feature maps\n",
    "--attr \"*\"                    # attributes list. \"*\" for all attributes\n",
    "\n",
    "# Network architecture\n",
    "--init_fm 32                  # number of feature maps in the first layer\n",
    "--max_fm 512                  # maximum number of feature maps\n",
    "--hid_dim 512                 # hidden layer size\n",
    "\n",
    "# Training parameters\n",
    "--v_flip False                # randomly flip images vertically (data augmentation)\n",
    "--h_flip True                 # randomly flip images horizontally (data augmentation)\n",
    "--batch_size 32               # batch size\n",
    "--optimizer \"adam,lr=0.0002\"  # optimizer\n",
    "--clip_grad_norm 5            # clip gradient L2 norm\n",
    "--n_epochs 1000               # number of epochs\n",
    "--epoch_size 50000            # number of images per epoch\n",
    "\n",
    "# Reload\n",
    "--reload \"\"                   # reload a trained classifier\n",
    "--debug False                 # debug mode (if True, load a small subset of the dataset)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (37316337.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    --img_sz 256                  # image size\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Main parameters\n",
    "--img_sz 256                  # image size\n",
    "--img_fm 3                    # number of feature maps\n",
    "--attr \"*\"                    # attributes list. \"*\" for all attributes\n",
    "\n",
    "# Network architecture\n",
    "--init_fm 32                  # number of feature maps in the first layer\n",
    "--max_fm 512                  # maximum number of feature maps\n",
    "--hid_dim 512                 # hidden layer size\n",
    "\n",
    "# Training parameters\n",
    "--v_flip False                # randomly flip images vertically (data augmentation)\n",
    "--h_flip True                 # randomly flip images horizontally (data augmentation)\n",
    "--batch_size 32               # batch size\n",
    "--optimizer \"adam,lr=0.0002\"  # optimizer\n",
    "--clip_grad_norm 5            # clip gradient L2 norm\n",
    "--n_epochs 1000               # number of epochs\n",
    "--epoch_size 50000            # number of images per epoch\n",
    "\n",
    "# Reload\n",
    "--reload \"\"                   # reload a trained classifier\n",
    "--debug False                 # debug mode (if True, load a small subset of the dataset)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Train a Fader Network\n",
    "You can train a Fader Network with train.py. The autoencoder can receive feedback from:\n",
    "- The image reconstruction loss\n",
    "- The latent discriminator loss\n",
    "- The PatchGAN discriminator loss\n",
    "- The classifier loss\n",
    "\n",
    "In the paper, only the first two losses are used, but the two others could improve the results further. You can tune the impact of each of these losses with the lambda_ae, lambda_lat_dis, lambda_ptc_dis, and lambda_clf_dis coefficients. Below is a complete list of all parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECOMMENT THIS PART TO USE THE \n",
    "if input(\"Do you want to train the model? (yes/no)\") == \"yes\":\n",
    "    %cd\n",
    "    %cd tests/MLA_Projet_2023/FaderNetworks_NIPS2017/\n",
    "    !chmod +x train.py\n",
    "    !python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "# Main parameters\n",
    "--img_sz 256                      # image size\n",
    "--img_fm 3                        # number of feature maps\n",
    "--attr \"Male\"                     # attributes list. \"*\" for all attributes\n",
    "\n",
    "# Networks architecture\n",
    "--instance_norm False             # use instance normalization instead of batch normalization\n",
    "--init_fm 32                      # number of feature maps in the first layer\n",
    "--max_fm 512                      # maximum number of feature maps\n",
    "--n_layers 6                      # number of layers in the encoder / decoder\n",
    "--n_skip 0                        # number of skip connections\n",
    "--deconv_method \"convtranspose\"   # deconvolution method\n",
    "--hid_dim 512                     # hidden layer size\n",
    "--dec_dropout 0                   # dropout in the decoder\n",
    "--lat_dis_dropout 0.3             # dropout in the latent discriminator\n",
    "\n",
    "# Training parameters\n",
    "--n_lat_dis 1                     # number of latent discriminator training steps\n",
    "--n_ptc_dis 0                     # number of PatchGAN discriminator training steps\n",
    "--n_clf_dis 0                     # number of classifier training steps\n",
    "--smooth_label 0.2                # smooth discriminator labels\n",
    "--lambda_ae 1                     # autoencoder loss coefficient\n",
    "--lambda_lat_dis 0.0001           # latent discriminator loss coefficient\n",
    "--lambda_ptc_dis 0                # PatchGAN discriminator loss coefficient\n",
    "--lambda_clf_dis 0                # classifier loss coefficient\n",
    "--lambda_schedule 500000          # lambda scheduling (0 to disable)\n",
    "--v_flip False                    # randomly flip images vertically (data augmentation)\n",
    "--h_flip True                     # randomly flip images horizontally (data augmentation)\n",
    "--batch_size 32                   # batch size\n",
    "--ae_optimizer \"adam,lr=0.0002\"   # autoencoder optimizer\n",
    "--dis_optimizer \"adam,lr=0.0002\"  # discriminator optimizer\n",
    "--clip_grad_norm 5                # clip gradient L2 norm\n",
    "--n_epochs 1000                   # number of epochs\n",
    "--epoch_size 50000                # number of images per epoch\n",
    "\n",
    "# Reload\n",
    "--ae_reload \"\"                    # reload pretrained autoencoder\n",
    "--lat_dis_reload \"\"               # reload pretrained latent discriminator\n",
    "--ptc_dis_reload \"\"               # reload pretrained PatchGAN discriminator\n",
    "--clf_dis_reload \"\"               # reload pretrained classifier\n",
    "--eval_clf \"\"                     # evaluation classifier (trained with classifier.py)\n",
    "--debug False                     # debug mode (if True, load a small subset of the dataset)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Main parameters\n",
    "--img_sz 256                      # image size\n",
    "--img_fm 3                        # number of feature maps\n",
    "--attr \"Male\"                     # attributes list. \"*\" for all attributes\n",
    "\n",
    "# Networks architecture\n",
    "--instance_norm False             # use instance normalization instead of batch normalization\n",
    "--init_fm 32                      # number of feature maps in the first layer\n",
    "--max_fm 512                      # maximum number of feature maps\n",
    "--n_layers 6                      # number of layers in the encoder / decoder\n",
    "--n_skip 0                        # number of skip connections\n",
    "--deconv_method \"convtranspose\"   # deconvolution method\n",
    "--hid_dim 512                     # hidden layer size\n",
    "--dec_dropout 0                   # dropout in the decoder\n",
    "--lat_dis_dropout 0.3             # dropout in the latent discriminator\n",
    "\n",
    "# Training parameters\n",
    "--n_lat_dis 1                     # number of latent discriminator training steps\n",
    "--n_ptc_dis 0                     # number of PatchGAN discriminator training steps\n",
    "--n_clf_dis 0                     # number of classifier training steps\n",
    "--smooth_label 0.2                # smooth discriminator labels\n",
    "--lambda_ae 1                     # autoencoder loss coefficient\n",
    "--lambda_lat_dis 0.0001           # latent discriminator loss coefficient\n",
    "--lambda_ptc_dis 0                # PatchGAN discriminator loss coefficient\n",
    "--lambda_clf_dis 0                # classifier loss coefficient\n",
    "--lambda_schedule 500000          # lambda scheduling (0 to disable)\n",
    "--v_flip False                    # randomly flip images vertically (data augmentation)\n",
    "--h_flip True                     # randomly flip images horizontally (data augmentation)\n",
    "--batch_size 32                   # batch size\n",
    "--ae_optimizer \"adam,lr=0.0002\"   # autoencoder optimizer\n",
    "--dis_optimizer \"adam,lr=0.0002\"  # discriminator optimizer\n",
    "--clip_grad_norm 5                # clip gradient L2 norm\n",
    "--n_epochs 1000                   # number of epochs\n",
    "--epoch_size 50000                # number of images per epoch\n",
    "\n",
    "# Reload\n",
    "--ae_reload \"\"                    # reload pretrained autoencoder\n",
    "--lat_dis_reload \"\"               # reload pretrained latent discriminator\n",
    "--ptc_dis_reload \"\"               # reload pretrained PatchGAN discriminator\n",
    "--clf_dis_reload \"\"               # reload pretrained classifier\n",
    "--eval_clf \"\"                     # evaluation classifier (trained with classifier.py)\n",
    "--debug False                     # debug mode (if True, load a small subset of the dataset)\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
